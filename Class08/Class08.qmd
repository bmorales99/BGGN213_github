---
title: "Class08: Breast Cancer Analysis Project"
author: "Bobbie Morales A15443382"
format: pdf
toc: true
---


## Background

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in the last class. We will extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. 

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass

## Data import 

```{r}
fna.data <- "WisconsinCancer (2).csv"
wisc.df <- read.csv(fna.data, row.names=1)
```
Make sure we do not include sample id or diagnosis columns in the data that we analyze below 

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
wisc.data <- wisc.df[,-1]
dim(wisc.data)
```

##Exploratory data analysis
>Q1. Q1. How many observations are in this dataset?

There are `r nrow(wisc.data)` observations/samples/patients in the data set
>Q2. How many of the observations have a malignant diagnosis?

```{r}
sum(wisc.df$diagnosis == "M")
```

```{r}
table(wisc.df$diagnosis)
```

>Q3. How many variables/features in the data are suffixed with _mean?

```{r}
#colnames(wisc.data)
length(grep("_mean", colnames(wisc.data)))
```

## Principal Component Analysis 

The main function in base R for PCA is called `prcomp()`. A optional argument `scale` should nearly always be switched to `scale = TRUE` for this function

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```


Let's make our main result figure - the "PC plot" or "score plot" "coordination plot" ..


```{r}
library(ggplot2)
ggplot(wisc.pr$x, aes(x = PC1, y = PC2, col = diagnosis)) +
geom_point()
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427 is captured by PC1, as seen by the proportion variance row.

```{r}
summary(wisc.pr)
```


>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

Three principal components are required, we see PC3 has a cumulative proportion of 0.72636.

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

Seven principal components are required, we see PC7 has a cumulative proportion of 0.91010.

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```

the plot in question 7 is crowded, we cannot see specific data points. We can not draw any conclusions from this plot. 

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

we can see 2 distinct clusters when we plot PC1 vs PC3. We can see that PC1 is separating malignant and benign. Compared to PC1 and PC2, this plot is shifted down, indicating that PC3 might have less of an affect. In PC1 vs PC2, there is a clear elbow point where we could cut the data to show two distinct groups.

```{r}
ggplot(wisc.pr$x, aes(col = diagnosis , 
     x = PC1, y = PC3)) + 
  geom_point()
```

## Variance explained

Below is the same as the variance explained above. This is how it would be done manually.

```{r}
pr.var <- wisc.pr$sdev^2
pve <- pr.var / sum(pr.var)
pve
```

```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
## Communicating PCA results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

 The component of the loading vector is -0.26085376.  

```{r}
wisc.pr$rotation[,1]
```

## Hierarchical clustering

scaling data

```{r}
data.scaled <- scale(wisc.data)
```

calculating distacne btw all pairs of observations in new scaled data set

```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage. 

```{r}
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
```
Results of hierarchical clustering^^

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

## combining PCA and Clustering

>Q12 Which method gives your favorite results for the same data.dist dataset? Explain your reasoning

The ward.D2 method gave the best results for the data since it was more clear and it allowed us to visualize 2 distinct groups and where they would be divided.


```{r}
d <- dist( wisc.pr$x[,1:7] )
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
abline(h=70, col="red")
```
Get my cluster membership vector
```{r}
grps<- cutree(wisc.pr.hclust, k=2)
table(grps)
```
>Q13. How well does the newly created model with four clusters separate out the two diagnoses?
 
 new hierarchical clustering model did a decent job separating benign and malignant samples with 2 clusters. Cluster 1 has 28 benign samples mixed in with malignant while cluster 2 has 24 malignant samples mixed in with benign. This would mean that they both have ~25 false results, which would result in giving a patient the wrong results. 
 with 4 clusters there is a lot of variance in the malignant results.

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
table(wisc.pr.hclust.clusters, diagnosis)
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters, diagnosis)
```
actual diagnoses

```{r}
table(diagnosis)
```

>14  How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses



when k=4, Cluster 1 containts the majority of the malignant samples while cluster 3 contains the benign smaples. The pca + clustering analysis did a better job of separating the diagnoses.
```{r}
table(wisc.hclust.clusters, diagnosis)
```



#sensitivity
TP: 179 
FP: 24

sensitivity: TP/(TP +FN)

section 4 answer Q14, (much better)